{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QZQsPqHvTlJY",
        "U8Jqe4Iz1NqV"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6aWWzarlKuA/367Ue9DWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archei2500/overdubbing_courses/blob/main/automatic_subtitles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏, –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å—É–±—Ç–∏—Ç—Ä–æ–≤!**\n",
        "\n",
        "> –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ:\n",
        "*   —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ –≤–∏–¥–µ–æ;\n",
        "*   —Å–æ–∑–¥–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏ (—Ñ–∞–π–ª SRT);\n",
        "*   –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—É—é —Ä–µ—á—å –∏–ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫.\n",
        "\n",
        "> –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.\n",
        "1.   –î–ª—è –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –∫ –ø–µ—Ä–≤–æ–º—É —Ä–∞–∑–¥–µ–ª—É, –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É. –í —Å—Ä–µ–¥—É —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã.\n",
        "2.   –ü–µ—Ä–µ–π–¥–∏—Ç–µ –∫–æ –≤—Ç–æ—Ä–æ–º—É —Ä–∞–∑–¥–µ–ª—É: –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ –≤ —Å–∏—Å—Ç–µ–º—É –ª—é–±—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö.\n",
        "3.   –ó–∞—Ç–µ–º –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª, –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –≤–∞—à–µ–π –∑–∞–¥–∞—á–µ–π, –∏ —Ä–∞–∑–≤–µ—Ä–Ω–∏—Ç–µ –µ–≥–æ.\n",
        "\n",
        "> *–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –±–µ–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–µ—Ä–≤—É—é —è—á–µ–π–∫—É. –≠—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ SRT —Ñ–∞–π–ª–∞ (–º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —É–∂–µ –≥–æ—Ç–æ–≤—ã–π —Å–≤–æ–π SRT —Ñ–∞–π–ª).*\n",
        "\n",
        "> ***–£–±–µ–¥–∏—Ç–µ—Å—å –≤ —Ç–æ–º, —á—Ç–æ –≤—ã –ø–æ–¥–∫–ª—é—á–µ–Ω—ã –∫ —Å—Ä–µ–¥–µ —Å GPU, –µ—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–æ–≤!!!***"
      ],
      "metadata": {
        "id": "4zYMZ_aS044B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã"
      ],
      "metadata": {
        "id": "gEtWFA-34yju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å Whisper –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π:\n",
        "\n",
        "#!pip install moviepy # –ø–æ—Å–º–æ—Ç—Ä–∏–º, –º–æ–∂–µ—Ç. –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å\n",
        "!pip install -U openai-whisper # —É—Å—Ç–∞–Ω–æ–≤–∫–∞ whisper\n",
        "!pip install pydub\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import clear_output # –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "from datetime import timedelta\n",
        "from google.colab import files\n",
        "import whisper\n",
        "from tensorflow.test import gpu_device_name\n",
        "\n",
        "\n",
        "def str_to_time(s):\n",
        "  return int(s[6:8]) + 60 * int(s[3:5]) + 3600 * int(s[:2]) + float('0.' + s[9:12])\n",
        "\n",
        "\n",
        "def time_to_str(time):\n",
        "  hours = int(time // 3600)\n",
        "  minutes = int((time - 3600 * hours) // 60)\n",
        "  seconds = time - 60 * minutes\n",
        "  str_sec = str(round(seconds, 3)).replace('.', ',')\n",
        "  return str(hours).zfill(2) + ':' + str(minutes).zfill(2) + ':' + str_sec[:str_sec.find(',')].zfill(2) + ',' + str_sec[str_sec.find(',') + 1:].ljust(3, '0')\n",
        "\n",
        "\n",
        "def write_result_to_file(res, path_to_text):\n",
        "  file = open(path_to_text, 'w')\n",
        "  text_massive = []\n",
        "  segments = res['segments']\n",
        "  for segment in segments:\n",
        "    fract1 = str(round(segment['start'] % 1, 3))\n",
        "    fract2 = str(round(segment['end'] % 1, 3))\n",
        "    start_time = str(0) + str(timedelta(seconds = int(segment['start']))) + ',' + fract1[fract1.find('.') + 1:].ljust(3, '0')\n",
        "    end_time = str(0) + str(timedelta(seconds = int(segment['end']))) + ',' + fract2[fract2.find('.') + 1:].ljust(3, '0')\n",
        "    text = segment['text']\n",
        "    id = segment['id']\n",
        "    seg = f\"{id + 1}\\n{start_time} --> {end_time}\\n{text[1:] if text and text[0] == ' ' else 'EMPTY' if text == '' else text}\"\n",
        "    text_massive.append(seg)\n",
        "  file.write(text_massive[0] + '\\n')\n",
        "  for txt_str in text_massive[1:-1]:\n",
        "    file.write('\\n' + txt_str + '\\n')\n",
        "  file.write('\\n' + text_massive[-1])\n",
        "  file.close()\n",
        "\n",
        "\n",
        "def write_words_to_file(res, path_to_text):\n",
        "  file = open(path_to_text, 'w')\n",
        "  text_massive = []\n",
        "  segments = res['segments']\n",
        "  index_shift = 0\n",
        "  for segment in segments:\n",
        "    words = segment['words']\n",
        "    for idx, word in enumerate(words):\n",
        "      fract1 = str(round(word['start'] % 1, 3))\n",
        "      fract2 = str(round(word['end'] % 1, 3))\n",
        "      start_time = str(0) + str(timedelta(seconds = int(word['start']))) + ',' + fract1[fract1.find('.') + 1:].ljust(3, '0')\n",
        "      end_time = str(0) + str(timedelta(seconds = int(word['end']))) + ',' + fract1[fract1.find('.') + 1:].ljust(3, '0')\n",
        "      text = word['word']\n",
        "      seg = f\"{index_shift + idx + 1}\\n{start_time} --> {end_time}\\n{text[1:] if text and text[0] == ' ' else 'EMPTY' if text == '' else text}\"\n",
        "      text_massive.append(seg)\n",
        "    index_shift += idx\n",
        "  file.write(text_massive[0] + '\\n')\n",
        "  for txt_str in text_massive[1:-1]:\n",
        "    file.write('\\n' + txt_str + '\\n')\n",
        "  file.write('\\n' + text_massive[-1])\n",
        "  file.close()\n",
        "\n",
        "\n",
        "def check_punctuation_percent(path, lang, subtitles):\n",
        "  punct = '.:!?,-‚Äî'\n",
        "  ok = True\n",
        "  punct_symbols = 0\n",
        "  other_symbols = 0\n",
        "\n",
        "  if not subtitles:\n",
        "    with open(path, 'r') as txtfile:\n",
        "      text = txtfile.read()\n",
        "    for char in text:\n",
        "      if char in punct:\n",
        "        punct_symbols += 1\n",
        "      else:\n",
        "        other_symbols += 1\n",
        "  else:\n",
        "    with open(path, 'r') as txtfile:\n",
        "      lines = [''] + txtfile.read().split('\\n')\n",
        "    for i in range(3, len(lines), 4):\n",
        "      for char in lines[i]:\n",
        "        if char in punct:\n",
        "          punct_symbols += 1\n",
        "        else:\n",
        "          other_symbols += 1\n",
        "\n",
        "  percent = punct_symbols / other_symbols * 100\n",
        "  if lang == 'en':\n",
        "    if percent < 1.3:\n",
        "      ok = False\n",
        "  elif lang == 'ru':\n",
        "    if percent < 2:\n",
        "      ok = False\n",
        "  return ok\n",
        "\n",
        "\n",
        "# –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≥–æ, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ —Å—Ä–µ–¥–µ GPU\n",
        "device_name = gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–æ–≤–µ—Ç: –ø–æ–¥–∫–ª—é—á–∏—Ç–µ—Å—å –∫ —Å—Ä–µ–¥–µ —Å GPU.')\n",
        "\n",
        "model_name = \"turbo\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\", \"turbo\"]\n",
        "# –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "# –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ\n",
        "yt_downloaded = False\n",
        "gd_mounted = False\n",
        "\n",
        "path_to_video = '/content/vid.mp4'\n",
        "\n",
        "!rm rf /content/sample_data # —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω–æ–π –ø–∞–ø–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—ë—Ç—Å—è –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "2Gb2S89T48bJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ"
      ],
      "metadata": {
        "id": "QLPoBz2U23Eq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vwSvZa2x0kFV"
      },
      "outputs": [],
      "source": [
        "# @title –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –µ—Å–ª–∏ –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ—Ç–µ Google –î–∏—Å–∫:\n",
        "#@markdown ### –°–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏\n",
        "upload_method = \"–° —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\" #@param [\"–° —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\", \"–ü–æ —Å—Å—ã–ª–∫–µ —Å YouTube\", \"–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É Google Drive\"]\n",
        "#@markdown <font color=\"orange\"> –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –∏ —Å–æ–≤–µ—Ç—ã:\n",
        "\n",
        "#@markdown <font color=\"orange\"> ``1. –ü—Ä–∏ –≤—ã–±–æ—Ä–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –Ω–∞–∂–∞—Ç—å –Ω–∞ –∫–Ω–æ–ø–∫—É –≤ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–±—Ä–∞—Ç—å —Ñ–∞–π–ª —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.``\n",
        "\n",
        "#@markdown <font color=\"orange\"> ``2. –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å YouTube —É–∫–∞–∂–∏—Ç–µ —Å—Å—ã–ª–∫—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ.``\n",
        "\n",
        "#@markdown <font color=\"orange\"> ``3. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Å–ø–æ—Å–æ–±–∞, —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ —Å Google –î–∏—Å–∫–æ–º, —Å–∏—Å—Ç–µ–º–∞ –ø–æ–ø—Ä–æ—Å–∏—Ç –≤–∞—Å –¥–∞—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç—É–ø –∫ –≤–∞—à–µ–º—É Google Drive. –ó–∞—Ç–µ–º –æ–Ω –±—É–¥–µ—Ç —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –≤ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º –ø–æ–ª–µ.``\n",
        "\n",
        "#@markdown ``–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫ —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã YouTube –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ `` üëá\n",
        "youtube_url = 'https://www.youtube.com/watch?v=YOUTUBE_ID' #@param {type:\"string\"}\n",
        "#@markdown <font color=\"lightgrey\"> ``(—Ñ–æ—Ä–º–∞—Ç –¥–ª—è Rutube: https://rutube.ru/video/VIDEO_ID/)``\n",
        "\n",
        "#@markdown ``–í–≤–µ–¥–∏—Ç–µ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ –Ω–∞ –≤–∞—à–µ–º Google –î–∏—Å–∫–µ (–¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π —Å Google Drive) `` üëá\n",
        "path_google_drive = '/content/drive/MyDrive/path_to_vid' #@param {type:\"string\"}\n",
        "\n",
        "from urllib import parse as urlparse\n",
        "\n",
        "# –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ —Ç–∞–∫–∏–µ —É–∂–µ –±—ã–ª–∏ –≤ –§–° –ø—Ä–µ–∂–¥–µ\n",
        "if os.path.isfile(path_to_video):\n",
        "    os.remove(path_to_video)\n",
        "if os.path.isfile('/content/aud.wav'):\n",
        "    os.remove('/content/aud.wav')\n",
        "\n",
        "path_to_video = '/content/vid.mp4' # –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "\n",
        "# –í–∏–¥–µ–æ—Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏\n",
        "if upload_method == \"–° —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\":\n",
        "  uploaded = files.upload()\n",
        "  for filename in uploaded.keys():\n",
        "    if '/content/' + filename != path_to_video:\n",
        "      os.rename(filename, path_to_video)  # –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
        "elif upload_method == \"–ü–æ —Å—Å—ã–ª–∫–µ —Å YouTube\":\n",
        "  if not yt_downloaded:\n",
        "    !pip install yt-dlp # —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ\n",
        "    yt_downloaded = True\n",
        "  url_data = urlparse.urlparse(youtube_url)\n",
        "  query = urlparse.parse_qs(url_data.query)\n",
        "  YOUTUBE_ID = query[\"v\"][0]\n",
        "  # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ —Å YouTube\n",
        "  !yt-dlp -f \"bestvideo[ext=mp4]+bestaudio[ext=wav]/best[ext=mp4]/best\" --output \"/content/vid.%(ext)s\" https://www.youtube.com/watch?v=$YOUTUBE_ID\n",
        "  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
        "  for file in os.listdir('/content'):\n",
        "    if file.startswith('vid'):\n",
        "        path_to_video = os.path.join('/content', file)\n",
        "else:\n",
        "  if not gd_mounted:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\") # –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Google –î–∏—Å–∫–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ\n",
        "    gd_mounted = True\n",
        "  if not os.path.isfile(path_google_drive): # –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω\n",
        "      print(\"ERROR: File not found!\")\n",
        "      raise SystemExit(0)\n",
        "  !cp $path_google_drive $path_to_video # –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å Google –î–∏—Å–∫–∞\n",
        "\n",
        "# –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∏ –∏–∑ –≤–∏–¥–µ–æ\n",
        "video = VideoFileClip(path_to_video)\n",
        "video.audio.write_audiofile('/content/aud.wav') # –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∏\n",
        "\n",
        "clear_output() # –æ—á–∏—Å—Ç–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ (+ –ø–µ—Ä–µ–≤–æ–¥ –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞)"
      ],
      "metadata": {
        "id": "QZQsPqHvTlJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏\n",
        "\n",
        "# @markdown –í–≤–µ–¥–∏—Ç–µ –ø–æ–¥—Å–∫–∞–∑–∫—É –¥–ª—è –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ —ç—Ç–æ –∑–∞ –ª–µ–∫—Ü–∏—è.\n",
        "prompt = 'The text below is a lecture on economics.' #@param {type: \"string\"}\n",
        "\n",
        "# @markdown –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á—å—é?\n",
        "download_file = False # @param {type: \"boolean\"}\n",
        "\n",
        "if os.path.isfile('/content/aud.wav'):\n",
        "  result = model.transcribe('/content/aud.wav', initial_prompt = prompt)\n",
        "  lang = result['language']\n",
        "\n",
        "  txt_file = open('/content/result.txt', 'w')\n",
        "  txt_file.write(result['text'])\n",
        "  txt_file.close()\n",
        "\n",
        "  clear_output()\n",
        "  # –≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º\n",
        "  if lang == 'ru' or lang == 'en':\n",
        "    ok = check_punctuation_percent('/content/result.txt', lang, False)\n",
        "    assert ok, f\" [!] –í–µ—Ä–æ—è—Ç–Ω–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∞ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.\"\n",
        "\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏ /content/result.txt')\n",
        "  if download_file:\n",
        "    files.download('/content/result.txt')\n",
        "else:\n",
        "  print('–í—ã –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –≤–∏–¥–µ–æ. –í–µ—Ä–Ω–∏—Ç–µ—Å—å –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —è—á–µ–π–∫–µ.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vo1YDDcUT15z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –ü–µ—Ä–µ–≤–æ–¥\n",
        "\n",
        "# @markdown –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏:\n",
        "path_to_text = '/content/result.txt' # @param {type: \"string\"}\n",
        "\n",
        "!pip install -U deep-translator\n",
        "!pip install iso639 # –¥–ª—è –∫–æ–¥–æ–≤ —è–∑—ã–∫–æ–≤\n",
        "from deep_translator import GoogleTranslator\n",
        "import iso639\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#@markdown –í–≤–µ–¥–∏—Ç–µ —è–∑—ã–∫, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –∂–µ–ª–∞–µ—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –∏–∑ –≤–∏–¥–µ–æ\n",
        "language = \"english\" #@param {type: \"string\"}\n",
        "language = language.lower()\n",
        "lang_capital = language[0].upper() + language[1 :]\n",
        "\n",
        "# –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —è–∑—ã–∫–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º\n",
        "langs_list = GoogleTranslator().get_supported_languages()\n",
        "if language in langs_list: # –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫ –µ—Å—Ç—å –≤ —ç—Ç–æ–º —Å–ø–∏—Å–∫–µ\n",
        "  language = iso639.to_iso639_1(lang_capital) # –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ —è–∑—ã–∫–∞ ISO639-1\n",
        "  path_to_tr_text = '/content/result_' + language + '.txt'\n",
        "  # —Å–æ–∑–¥–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ\n",
        "  output_file = open(path_to_tr_text, \"w\")\n",
        "  txtfile = open(path_to_text, \"r\")\n",
        "  text = txtfile.read()\n",
        "  # –ø–µ—Ä–µ–≤–æ–¥\n",
        "  translated = GoogleTranslator(source='auto', target = language).translate(text)\n",
        "  output_file.write(translated)\n",
        "  txtfile.close()\n",
        "  output_file.close()\n",
        "  clear_output()\n",
        "\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏ ', path_to_tr_text)\n",
        "  print('–û–Ω –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ –≤–∞—à –∫–æ–º–ø—å—é—Ç–µ—Ä.')\n",
        "  # –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä\n",
        "  files.download(path_to_tr_text)\n",
        "else:\n",
        "  print(\"–î–∞–Ω–Ω—ã–π —è–∑—ã–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s0pJrvdicTYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ - —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ (+ –∏—Ö –ø–µ—Ä–µ–≤–æ–¥)"
      ],
      "metadata": {
        "id": "O0NPwPS-edjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏\n",
        "\n",
        "# @markdown –í–≤–µ–¥–∏—Ç–µ –ø–æ–¥—Å–∫–∞–∑–∫—É –¥–ª—è –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ —ç—Ç–æ –∑–∞ –ª–µ–∫—Ü–∏—è. –ò–ª–∏ –∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–æ–≤.\n",
        "prompt = 'The text below, consisting of segments consisting of segments ‚Äî separate complete sentences, is a lecture on economics.' #@param {type: \"string\"}\n",
        "# @markdown –ù—É–∂–Ω—ã –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤?\n",
        "\n",
        "# @markdown <font color=\"orange\"> –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: ``–≠—Ç–æ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –æ–±–µ—Å–ø–µ—á–∏—Ç –±–æ–ª—å—à—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤ —Å—É–±—Ç–∏—Ç—Ä–∞—Ö.``\n",
        "word_timestamps = True # @param {type: \"boolean\"}\n",
        "# @markdown –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á—å—é?\n",
        "download_file = False # @param {type: \"boolean\"}\n",
        "\n",
        "if os.path.isfile('/content/aud.wav'):\n",
        "  # temperature=(0.0, 0.2, 0.4, 0.6) - –º–æ–∂–Ω–æ –ø–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏, –ø–µ—Ä–µ–¥–∞–≤ –∞—Ä–≥—É–º–µ–Ω—Ç –≤ –º–µ—Ç–æ–¥ –Ω–∏–∂–µ\n",
        "  result = model.transcribe('/content/aud.wav', initial_prompt = prompt, word_timestamps = word_timestamps)\n",
        "  lang = result['language']\n",
        "\n",
        "  write_result_to_file(result, '/content/result.srt')\n",
        "  if word_timestamps:\n",
        "    path_to_words = '/content/words.txt'\n",
        "    write_words_to_file(result, path_to_words)\n",
        "    word_lines = []\n",
        "    with open(path_to_words, 'r') as wf:\n",
        "      word_lines = [''] + wf.read().split('\\n')\n",
        "\n",
        "  if lang == 'ru' or lang == 'en':\n",
        "    ok = check_punctuation_percent('/content/result.srt', lang, True)\n",
        "    assert ok, f\" [!] –í–µ—Ä–æ—è—Ç–Ω–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∞ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.\"\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏ /content/result.srt')\n",
        "  if download_file:\n",
        "    files.download('/content/result.srt')\n",
        "else:\n",
        "  print('–í—ã –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –≤–∏–¥–µ–æ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–µ—Ä–Ω–∏—Ç–µ—Å—å –∫ —è—á–µ–π–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6yy4kP5yekXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "\n",
        "#  @markdown –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã:\n",
        "file_path = '/content/result.srt' # @param {type: \"string\"}\n",
        "# @markdown –í–≤–µ–¥–∏—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∞–µ–º–æ–π —Ñ—Ä–∞–∑—ã (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö):\n",
        "max_dur = 17 # @param {type: \"integer\"}\n",
        "# @markdown –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–∞–π–ª —Å–æ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫?\n",
        "word_timestamps = True # @param {type: \"boolean\"}\n",
        "# @markdown –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏?\n",
        "download_file = False # @param {type: \"boolean\"}\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# –≤–∞–∂–Ω—ã–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –Ω–∏–∂–µ–ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π\n",
        "part_1 = 12 # –¥–æ –∫–∞–∫–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å —Ç–∞–π–º–∏–Ω–≥–∞ (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
        "part_2 = 17 # —Å –∫–∞–∫–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å —Ç–∞–π–º–∏–Ω–≥–∞\n",
        "\n",
        "\n",
        "# —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ —Å–ª–æ–≤\n",
        "def compromise_time(time1, time2, s1, s2):\n",
        "  coef = len(s1) / len(s1 + s2)\n",
        "  return time_to_str(str_to_time(time1) + (str_to_time(time2) - str_to_time(time1)) * coef)\n",
        "\n",
        "\n",
        "# –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –ø–æ —Å–ª–æ–≤–∞–º\n",
        "def words_time(time1, time2, s1, s2, word_lines, last):\n",
        "  time1_time = str_to_time(time1)\n",
        "  time2_time = str_to_time(time2)\n",
        "  space1 = s1.rfind(' ')\n",
        "  space2 = s2.find(' ')\n",
        "  word1 = s1[space1 + 1:] if space1 != -1 else s1\n",
        "  word2 = s2[:space2] if space2 != -1 else s2\n",
        "  # –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª—É—á–∞–π, –≤ —Ñ–∞–π–ª–µ —Å–æ —Å–ª–æ–≤–∞–º–∏ –æ–Ω–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã\n",
        "  if '-' in word1:\n",
        "    word1 = s1[s1.rfind('-'):]\n",
        "  if '-' in word2:\n",
        "    word2 = s2[:s2.find('-')]\n",
        "  print(word1)\n",
        "  print(word2)\n",
        "\n",
        "  new_end = 0\n",
        "  new_start = 0\n",
        "\n",
        "  for i in range(0, len(word_lines), 4):\n",
        "    if i + 1 < len(word_lines):\n",
        "      if word_lines[i+3] == word1 and word_lines[i+7] == word2:\n",
        "        # –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª–æ–≤ –¥–∞–Ω–Ω–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É\n",
        "        start_2_word_time = str_to_time(word_lines[i+6][:part_1])\n",
        "        if start_2_word_time > time1_time:\n",
        "          end_1_word_time = str_to_time(word_lines[i+2][part_2:])\n",
        "          # –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª—É—á–∞–π, —á—Ç–æ –∫–æ–Ω–µ—Ü –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –±—É–¥–µ—Ç –ø–æ–∑–∂–µ –Ω–∞—á–∞–ª–∞ –≤—Ç–æ—Ä–æ–≥–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º\n",
        "          if end_1_word_time > start_2_word_time:\n",
        "            start_1_word_time = str_to_time(word_lines[i+2][:part_1])\n",
        "            new_end = start_1_word_time\n",
        "          else:\n",
        "            new_end = end_1_word_time\n",
        "          new_start = start_2_word_time\n",
        "          # –æ–±—Ä–µ–∑–∞–µ–º –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞\n",
        "          # –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ —ç—Ç–∞ —á–∞—Å—Ç—å –ø—Ä–µ–ª–æ–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω—è—è\n",
        "          if last:\n",
        "              words_num = len(s2.split()) # –ø–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤\n",
        "              word_lines = word_lines[i + (words_num + 1) * 4:]\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "  if not new_end:\n",
        "    print('–í–æ–∑–Ω–∏–∫ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª—É—á–∞–π. –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –ø–æ–ø–∞–ª–æ—Å—å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Å–ª–æ–≤–æ!')\n",
        "    print('–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤—Ä—É—á–Ω—É—é –º–µ–∂–¥—É ', time1, ' –∏ ', time2)\n",
        "    print('–í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω—É–ª–µ–≤—ã–º–∏.')\n",
        "\n",
        "  return time_to_str(new_end), time_to_str(new_start), word_lines\n",
        "\n",
        "\n",
        "# # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞\n",
        "# def proc_segm(new_lines, lines, current_sentence, start_timing, id, start, end, max_dur, ind, i):\n",
        "#   # –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –≤—ã—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥. –ø—Ä–µ–¥–ª. –∏ –Ω–æ–≤–æ–µ –¥–æ —Ç–æ—á–∫–∏\n",
        "#   if str_to_time(end) - str_to_time(start_timing) <= max_dur:\n",
        "#     current_sentence += ' ' + lines[i + 3][:ind + 1]\n",
        "#     new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + end + '\\n' + current_sentence)\n",
        "#     id += 1\n",
        "#   # –∏–Ω–∞—á–µ –Ω—É–∂–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –ø—Ä–µ–¥. –ø—Ä–µ–¥–ª. –∏ –∑–∞—Ç–µ–º —Ç–æ –¥–æ —Ç–æ—á–∫–∏\n",
        "#   else:\n",
        "#     new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + lines[i-2][part_2:] + '\\n' + current_sentence)  # –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–∞—Å—Ç–∏\n",
        "#     new_lines.append('\\n' + str(id + 1) + '\\n' + lines[i+2][:part_1] + ' --> ' + end + '\\n' + lines[i + 3][:ind + 1])\n",
        "#     id += 2\n",
        "#   # –±–µ—Ä—ë—Ç—Å—è –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "#   current_sentence = lines[i + 3][ind + 2:]\n",
        "#   start_timing = start\n",
        "\n",
        "#   return new_lines, current_sentence, start_timing, id\n",
        "\n",
        "\n",
        "def sep_segment(str, start, end, symbols, count, limit, word_lines=None):\n",
        "  sent_array = [] # –º–∞—Å—Å–∏–≤ –∫–æ—Ä—Ç–µ–∂–µ–π –∏–∑ —Ç–∞–π–º–∏–Ω–≥–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
        "  buff1 = \"\"\n",
        "  buff2 = \"\"\n",
        "  last = False\n",
        "  for i in range(count): # –µ—Å—Ç—å –æ–ø–∞—Å–Ω–æ—Å—Ç—å –¥–æ–π—Ç–∏ –¥–æ –∫–æ–Ω—Ü–∞\n",
        "    index = min((str.find(char) for char in symbols if char in str)) #default=-1\n",
        "    if i == count - 1: # –æ—Ç–º–µ—Ç–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å, –Ω–∞ –∫–æ—Ç–æ—Ä—É—é —Ä–∞–∑–¥–µ–ª–∏–ª—Å—è —Å–µ–≥–º–µ–Ω—Ç\n",
        "        last = True\n",
        "    if index != len(str) - 1: # –µ—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–º –æ–∫–∞–∑–∞–ª—Å—è –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
        "      buff1 = str[:index + 1]\n",
        "      buff2 = str[index + 2:]\n",
        "      if word_lines:\n",
        "        # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª —Å–æ —Å–ª–æ–≤–∞–º–∏\n",
        "        new_end, new_start, word_lines = words_time(start, end, buff1, buff2, word_lines, last)\n",
        "      else:\n",
        "        # —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –º–µ—Ç–∫—É\n",
        "        pass # –ø–æ–∑–∂–µ —ç—Ç–æ –¥–æ–±–∞–≤–ª—é\n",
        "      sent_array.append((start + ' --> ' + new_end, buff1)) # –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º\n",
        "      start = new_start\n",
        "      str = buff2\n",
        "    else:\n",
        "      sent_array.append((new_start + ' --> ' + end, str)) # –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º\n",
        "      str = ''\n",
        "  # —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –∫–æ–Ω—á–∞–µ—Ç—Å—è –∑–Ω–∞–∫–æ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "  if str:\n",
        "      sent_array.append((new_start + ' --> ' + end, str))\n",
        "\n",
        "  return sent_array\n",
        "\n",
        "\n",
        "def replace_consecutive_punctuation(input_string):\n",
        "    pattern = r'([!?.])\\1+'\n",
        "    replaced_string = re.sub(pattern, lambda x: x.group()[0], input_string)\n",
        "    return replaced_string\n",
        "\n",
        "\n",
        "# —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏\n",
        "def process_text(file_path, max_sent_dur, word_lines=None):\n",
        "    with open(file_path, 'r') as txtfile:\n",
        "        lines = [''] + txtfile.read().split('\\n')\n",
        "        print(lines)\n",
        "\n",
        "    new_lines = []\n",
        "    current_sentence = \"\"\n",
        "    symbols = '.!?'\n",
        "    start_timing = ''\n",
        "\n",
        "    id = 1\n",
        "\n",
        "    for i in range(0, len(lines), 4):\n",
        "      if i + 1 < len(lines):\n",
        "        time_match = re.match(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', lines[i+2])\n",
        "        text_match = re.match(r'.*[.?!]$', lines[i+3])\n",
        "\n",
        "        # –æ–±—Ä–∞–±–æ—Ç–∫–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª—É—á–∞—è —Å –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ–º –∏ –ø–æ–¥–æ–±–Ω—ã—Ö, –∫–æ–≥–¥–∞ –ø–æ–¥—Ä—è–¥ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
        "        lines[i+3] = replace_consecutive_punctuation(lines[i+3])\n",
        "        # –ü–æ–¥—Å—á—ë—Ç –∑–Ω–∞–∫–æ–≤ –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "        punct_ct = sum(lines[i+3].count(char) for char in symbols)\n",
        "\n",
        "        if time_match:\n",
        "          # –ø–µ—Ä–≤—ã–π —Å–ª—É—á–∞–π\n",
        "          if text_match and not start_timing: # —Ü–µ–ª–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ –ø–æ—Ç–µ—Ä—è–Ω–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–Ω–µ –±—ã–ª–æ –Ω–∞—á–∞–ª–∞)\n",
        "              if punct_ct > 1:  # –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Å–æ—Å—Ç–∞–≤–µ 2 –∏ –±–æ–ª–µ–µ\n",
        "                  if str_to_time(lines[i+2][part_2:]) - str_to_time(lines[i+2][:part_1]) <= max_sent_dur:\n",
        "                    new_lines.append('\\n' + str(id) + '\\n' + lines[i + 2] + '\\n' + lines[i+3])  # –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è\n",
        "                    id += 1\n",
        "                  else: # —É–∂–µ –Ω–∞–¥–æ –¥–µ–ª–∏—Ç—å, —Ç–∞–∫ –∫–∞–∫ –Ω–µ —É–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –≤ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "                    sent_arr = sep_segment(lines[i+3], lines[i+2][:part_1], lines[i+2][part_2:], symbols, punct_ct,\n",
        "                                           max_sent_dur, word_lines)\n",
        "                    buff = ''\n",
        "                    for timing, sentence in sent_arr:\n",
        "                        if not buff:\n",
        "                            buff = sentence\n",
        "                            start_buff = timing[:part_1]\n",
        "                            end_buff = timing[part_2:]\n",
        "                            continue\n",
        "                        if str_to_time(timing[part_2:]) - str_to_time(start_buff) <= max_sent_dur:\n",
        "                            buff += ' ' + sentence\n",
        "                            end_buff = timing[part_2:]\n",
        "                        else:\n",
        "                            new_lines.append('\\n' + str(id) + '\\n' + start_buff + ' --> ' + end_buff + '\\n' + buff)\n",
        "                            id += 1\n",
        "                            buff = ''\n",
        "                    if buff: # –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ü–∏–∫–ª–∞ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ–¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
        "                        new_lines.append('\\n' + str(id) + '\\n' + start_buff + ' --> ' + end_buff + '\\n' + buff)\n",
        "                        id += 1\n",
        "              else: # —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
        "                  new_lines.append('\\n' + str(id) + '\\n' + lines[i+2] + '\\n' + lines[i+3])  # –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è\n",
        "                  id += 1\n",
        "\n",
        "          # –≤—Ç–æ—Ä–æ–π —Å–ª—É—á–∞–π - –∫–æ–Ω—Ü–æ–≤–∫–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å\n",
        "          elif text_match and start_timing:\n",
        "              # –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ, —á—Ç–æ –º—ã –º–æ–∂–µ–º –ø—Ä–∏–±–∞–≤–∏—Ç—å - –±—É–¥–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
        "              if str_to_time(lines[i+2][part_2:]) - str_to_time(start_timing) <= max_sent_dur:\n",
        "                  new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + lines[i + 2][part_2:] + '\\n' +\n",
        "                                   current_sentence + ' ' + lines[i+3])\n",
        "                  id += 1\n",
        "              else:\n",
        "                  if punct_ct > 1:\n",
        "                      sent_arr = sep_segment(lines[i + 3], lines[i + 2][:part_1], lines[i + 2][part_2:], symbols,\n",
        "                                             punct_ct, max_sent_dur, word_lines)\n",
        "                      end_buff = lines[i-2][part_2:]  # –≤—Ä–µ–º—è –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞\n",
        "                      for timing, sentence in sent_arr:\n",
        "                          if not current_sentence:\n",
        "                              current_sentence = sentence\n",
        "                              start_timing = timing[:part_1]\n",
        "                              end_buff = timing[part_2:]\n",
        "                              continue\n",
        "                          if str_to_time(timing[part_2:]) - str_to_time(start_timing) <= max_sent_dur:\n",
        "                              current_sentence += ' ' + sentence\n",
        "                              end_buff = timing[part_2:]\n",
        "                          else:\n",
        "                              new_lines.append(\n",
        "                                  '\\n' + str(id) + '\\n' + start_timing + ' --> ' + end_buff + '\\n' + current_sentence)\n",
        "                              id += 1\n",
        "                              current_sentence = ''\n",
        "                              start_timing = ''\n",
        "                      if current_sentence:  # –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ü–∏–∫–ª–∞ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ–¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
        "                          new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + end_buff + '\\n' +\n",
        "                                           current_sentence)\n",
        "                          id += 1\n",
        "                  else:\n",
        "                      new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' +\n",
        "                                       lines[i-2][part_2:] + '\\n' + current_sentence)  # –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–∞—Å—Ç–∏\n",
        "                      id += 1\n",
        "                      # –¥–æ–±–∞–≤–ª—è–µ–º —ç—Ç—É —á–∞—Å—Ç—å\n",
        "                      new_lines.append('\\n' + str(id) + '\\n' + lines[i+2][:part_1] + ' --> ' + lines[i+2][part_2:] +\n",
        "                                       '\\n' + lines[i+3])\n",
        "              current_sentence = ''\n",
        "              start_timing = ''\n",
        "\n",
        "          # —Ç—Ä–µ—Ç–∏–π —Å–ª—É—á–∞–π - —Å—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç—å, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å\n",
        "          elif start_timing:\n",
        "              if punct_ct > 0:\n",
        "                  # –¥–µ–ª–∏–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è / —á–∞—Å—Ç–∏\n",
        "                  sent_arr = sep_segment(lines[i + 3], lines[i + 2][:part_1], lines[i + 2][part_2:], symbols, punct_ct,\n",
        "                                         max_sent_dur, word_lines)\n",
        "                  #print(sent_arr)\n",
        "                  # –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é –∑–∞–≤–µ—Ä—à–∞—é—â—É—é —á–∞—Å—Ç—å\n",
        "                  if str_to_time(sent_arr[0][0][part_2:]) - str_to_time(start_timing) <= max_sent_dur:\n",
        "                      new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + sent_arr[0][0][part_2:] + '\\n'\n",
        "                                       + current_sentence + ' ' + sent_arr[0][1])\n",
        "                      id += 1\n",
        "                  else:\n",
        "                      new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + lines[i-2][part_2:] + '\\n'\n",
        "                                       + current_sentence)\n",
        "                      new_lines.append('\\n' + str(id + 1) + '\\n' + sent_arr[0][0] + '\\n' + sent_arr[0][1])\n",
        "                      id += 2\n",
        "                  sent_arr.pop(0)  # –∏–∑ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —É–¥–∞–ª—è–µ–º —ç—Ç—É —á–∞—Å—Ç—å\n",
        "                  current_sentence = ''\n",
        "                  start_timing = ''\n",
        "                  # –¥–∞–ª–µ–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è, –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∑–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–ª—É—á–∞–π\n",
        "                  if len(sent_arr) > 1: # –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–æ—Å—å –±–æ–ª—å—à–µ 1 —á–∞—Å—Ç–∏\n",
        "                      for timing, sentence in sent_arr[:-1]:\n",
        "                          if not current_sentence:\n",
        "                              current_sentence = sentence\n",
        "                              start_timing = timing[:part_1]\n",
        "                              end_buff = timing[part_2:]\n",
        "                              continue\n",
        "                          if str_to_time(timing[part_2:]) - str_to_time(start_timing) <= max_sent_dur:\n",
        "                              current_sentence += ' ' + sentence\n",
        "                              end_buff = timing[part_2:]\n",
        "                          else:\n",
        "                              new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + end_buff + '\\n' +\n",
        "                                               current_sentence)\n",
        "                              id += 1\n",
        "                              current_sentence = ''\n",
        "                              start_timing = ''\n",
        "                      if current_sentence:  # –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ü–∏–∫–ª–∞ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ–¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
        "                          new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + end_buff + '\\n' +\n",
        "                                           current_sentence)\n",
        "                          id += 1\n",
        "                  # –±–µ—Ä—ë–º –∫–∞–∫ –Ω–∞—á–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç\n",
        "                  current_sentence = sent_arr[-1][1]\n",
        "                  start_timing = sent_arr[-1][0][:part_1]\n",
        "              else:\n",
        "                  if str_to_time(lines[i+2][part_2:]) - str_to_time(start_timing) <= max_sent_dur:\n",
        "                      current_sentence += ' ' + lines[i+3]\n",
        "                  else:  # –Ω–æ —Ñ—Ä–∞–∑–∞ –æ–∫–∞–∑–∞–∑–∞–ª–∞—Å—å –¥–ª–∏–Ω–Ω–µ–µ, —á–µ–º –Ω–∞–¥–æ => –Ω–µ –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å\n",
        "                      new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' +\n",
        "                                       lines[i-2][part_2:] + '\\n' + current_sentence)  # –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–∞—Å—Ç–∏\n",
        "                      id += 1\n",
        "                      # –ë—É–¥–µ–º –ø—Ä–∏–±–∞–≤–ª—è—Ç—å –∫ —Ç–µ–∫—É—â–µ–º—É —Å–µ–≥–º–µ–Ω—Ç—É –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ\n",
        "                      start_timing = lines[i+2][:part_1]\n",
        "                      current_sentence = lines[i+3]\n",
        "\n",
        "          # –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª—É—á–∞–π - –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "          else:\n",
        "              if punct_ct > 0:  # –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 1 —Ç–∞–∫–æ–π —Å–∏–º–≤–æ–ª\n",
        "                  sent_arr = sep_segment(lines[i+3], lines[i+2][:part_1], lines[i+2][part_2:], symbols, punct_ct,\n",
        "                                         max_sent_dur, word_lines)\n",
        "                  buff = ''\n",
        "                  for timing, sentence in sent_arr:\n",
        "                      if not buff:\n",
        "                          buff = sentence\n",
        "                          start_buff = timing[:part_1]\n",
        "                          end_buff = timing[part_2:]\n",
        "                          continue\n",
        "                      if str_to_time(timing[part_2:]) - str_to_time(start_buff) <= max_sent_dur and (sentence.endswith('.') or sentence.endswith('!') or sentence.endswith('?')):\n",
        "                          buff += ' ' + sentence\n",
        "                          end_buff = timing[part_2:]\n",
        "                      else:\n",
        "                          new_lines.append('\\n' + str(id) + '\\n' + start_buff + ' --> ' + end_buff + '\\n' + buff)\n",
        "                          id += 1\n",
        "                          buff = ''\n",
        "                  if buff:  # –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ü–∏–∫–ª–∞ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ–¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
        "                      new_lines.append('\\n' + str(id) + '\\n' + start_buff + ' --> ' + end_buff + '\\n' + buff)\n",
        "                      id += 1\n",
        "                  current_sentence = sent_arr[-1][1]\n",
        "                  start_timing = sent_arr[-1][0][:part_1]  # –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å —Ç–∞–π–º–∏–Ω–≥–∞ –∏–∑ –∫–æ—Ä—Ç–µ–∂–∞\n",
        "              else:\n",
        "                  start_timing = lines[i+2][:part_1]  # –Ω–∞—á–∞–ª–æ —Ç–∞–π–º–∏–Ω–≥–∞, –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ\n",
        "                  current_sentence = lines[i+3]\n",
        "\n",
        "    return new_lines\n",
        "\n",
        "\n",
        "path_to_words = '/content/words.txt'\n",
        "word_lines = []\n",
        "if word_timestamps:\n",
        "  with open(path_to_words, 'r') as wf:\n",
        "    word_lines = [''] + wf.read().split('\\n')\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "new_lines = process_text(file_path, max_dur, word_lines)\n",
        "\n",
        "txt_file = open('/content/subtitles.srt', \"w\")\n",
        "txt_file.write(new_lines[0][1:] + '\\n') # –±–µ–∑ —Å–∏–º–≤–æ–ª–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É\n",
        "if len(new_lines) > 1:\n",
        "  for line in new_lines[1:-1]:\n",
        "      txt_file.write(line + '\\n')\n",
        "  txt_file.write(new_lines[-1])\n",
        "txt_file.close()\n",
        "\n",
        "print('–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ /content/subtitles.srt')\n",
        "if download_file:\n",
        "  files.download('/content/subtitles.srt')"
      ],
      "metadata": {
        "id": "iedlpqdnkocO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –ü–µ—Ä–µ–≤–æ–¥\n",
        "\n",
        "# @markdown –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏:\n",
        "path_to_text = '/content/subtitles.srt' # @param {type: \"string\"}\n",
        "\n",
        "!pip install -U deep-translator\n",
        "!pip install iso639 # –¥–ª—è –∫–æ–¥–æ–≤ —è–∑—ã–∫–æ–≤\n",
        "from deep_translator import GoogleTranslator\n",
        "import iso639\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "\n",
        "#@markdown –í–≤–µ–¥–∏—Ç–µ —è–∑—ã–∫, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –∂–µ–ª–∞–µ—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –∏–∑ –≤–∏–¥–µ–æ\n",
        "language = \"english\" #@param {type: \"string\"}\n",
        "language = language.lower()\n",
        "lang_capital = language[0].upper() + language[1 :]\n",
        "\n",
        "# –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —è–∑—ã–∫–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º\n",
        "langs_list = GoogleTranslator().get_supported_languages()\n",
        "if language in langs_list: # –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫ –µ—Å—Ç—å –≤ —ç—Ç–æ–º —Å–ø–∏—Å–∫–µ\n",
        "  language = iso639.to_iso639_1(lang_capital) # –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ —è–∑—ã–∫–∞ ISO639-1\n",
        "  path_to_tr_text = '/content/subtitles_' + language + '.srt'\n",
        "  # —Å–æ–∑–¥–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ\n",
        "  output_file = open(path_to_tr_text, \"w\")\n",
        "  txtfile = open(path_to_text, \"r\")\n",
        "  lines = [''] + txtfile.read().split('\\n')\n",
        "  for i, line in enumerate(lines):\n",
        "    if i != len(lines) - 1:\n",
        "      if i % 4 == 3:\n",
        "        translated = GoogleTranslator(source='auto', target = language).translate(line)\n",
        "        output_file.write(translated + \"\\n\")\n",
        "      else:\n",
        "        if i != 0:\n",
        "          output_file.write(line + \"\\n\")\n",
        "    else:\n",
        "      translated = GoogleTranslator(source='auto', target = language).translate(line)\n",
        "      output_file.write(translated)\n",
        "  txtfile.close()\n",
        "  output_file.close()\n",
        "  clear_output()\n",
        "\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏', path_to_tr_text)\n",
        "  print('–û–Ω –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ –≤–∞—à –∫–æ–º–ø—å—é—Ç–µ—Ä.')\n",
        "  files.download(path_to_tr_text)\n",
        "else:\n",
        "  print(\"–î–∞–Ω–Ω—ã–π —è–∑—ã–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º.\")"
      ],
      "metadata": {
        "id": "D1hhwyma0A55",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ —Ç–∞–π–º–∏–Ω–≥–æ–≤ –≤ —Å—É–±—Ç–∏—Ç—Ä–∞—Ö –ø–æ—Å–ª–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "# @markdown –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É:\n",
        "path_to_text = '/content/subtitles.srt' # @param {type: \"string\"}\n",
        "\n",
        "txt_file = open(path_to_text, 'r')\n",
        "lines = [''] + txt_file.read().split('\\n')\n",
        "txt_file.close()\n",
        "\n",
        "for i in range(len(lines)):\n",
        "  if i % 4 == 1:\n",
        "    lines[i] = str(int(i / 4 + 1))\n",
        "txt_file = open(path_to_text, 'w')\n",
        "for line in lines[1:-1]:\n",
        "  txt_file.write(line + '\\n')\n",
        "txt_file.write(lines[-1])\n",
        "txt_file.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FkV2DduoS675"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –†–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å Google Drive"
      ],
      "metadata": {
        "id": "U8Jqe4Iz1NqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "0-ykmtHc1X6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}