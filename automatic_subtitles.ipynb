{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QZQsPqHvTlJY",
        "O0NPwPS-edjC",
        "U8Jqe4Iz1NqV"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiV8HoJOmtlDCl8kBPgrVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archei2500/overdubbing_courses/blob/main/automatic_subtitles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏, –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å—É–±—Ç–∏—Ç—Ä–æ–≤!**\n",
        "\n",
        "> –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ:\n",
        "*   —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ –≤–∏–¥–µ–æ;\n",
        "*   —Å–æ–∑–¥–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏ (—Ñ–∞–π–ª SRT);\n",
        "*   –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—É—é —Ä–µ—á—å –∏–ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫.\n",
        "\n",
        "> –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.\n",
        "1.   –î–ª—è –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –∫ –ø–µ—Ä–≤–æ–º—É —Ä–∞–∑–¥–µ–ª—É, –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É. –í —Å—Ä–µ–¥—É —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã.\n",
        "2.   –ü–µ—Ä–µ–π–¥–∏—Ç–µ –∫–æ –≤—Ç–æ—Ä–æ–º—É —Ä–∞–∑–¥–µ–ª—É: –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ –≤ —Å–∏—Å—Ç–µ–º—É –ª—é–±—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö.\n",
        "3.   –ó–∞—Ç–µ–º –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª, –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –≤–∞—à–µ–π –∑–∞–¥–∞—á–µ–π, –∏ —Ä–∞–∑–≤–µ—Ä–Ω–∏—Ç–µ –µ–≥–æ.\n",
        "\n",
        "> *–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –±–µ–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–µ—Ä–≤—É—é —è—á–µ–π–∫—É. –≠—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ SRT —Ñ–∞–π–ª–∞ (–º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —É–∂–µ –≥–æ—Ç–æ–≤—ã–π —Å–≤–æ–π SRT —Ñ–∞–π–ª).*\n",
        "\n",
        "> ***–£–±–µ–¥–∏—Ç–µ—Å—å –≤ —Ç–æ–º, —á—Ç–æ –≤—ã –ø–æ–¥–∫–ª—é—á–µ–Ω—ã –∫ —Å—Ä–µ–¥–µ —Å GPU, –µ—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–æ–≤!!!***"
      ],
      "metadata": {
        "id": "4zYMZ_aS044B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã"
      ],
      "metadata": {
        "id": "gEtWFA-34yju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å Whisper –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π:\n",
        "\n",
        "#!pip install moviepy # –ø–æ—Å–º–æ—Ç—Ä–∏–º, –º–æ–∂–µ—Ç. –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å\n",
        "!pip install -U openai-whisper # —É—Å—Ç–∞–Ω–æ–≤–∫–∞ whisper\n",
        "!pip install pydub\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import clear_output # –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "from datetime import timedelta\n",
        "from google.colab import files\n",
        "import whisper\n",
        "from tensorflow.test import gpu_device_name\n",
        "\n",
        "\n",
        "def str_to_time(s):\n",
        "  return int(s[6:8]) + 60 * int(s[3:5]) + 3600 * int(s[:2]) + float('0.' + s[9:12])\n",
        "\n",
        "\n",
        "def time_to_str(time):\n",
        "  hours = int(time // 3600)\n",
        "  minutes = int((time - 3600 * hours) // 60)\n",
        "  seconds = time - 60 * minutes\n",
        "  str_sec = str(round(seconds, 3)).replace('.', ',')\n",
        "  return str(hours).zfill(2) + ':' + str(minutes).zfill(2) + ':' + str_sec[:str_sec.find(',')].zfill(2) + ',' + str_sec[str_sec.find(',') + 1:].ljust(3, '0')\n",
        "\n",
        "\n",
        "def write_result_to_file(res, path_to_text):\n",
        "  file = open(path_to_text, 'w')\n",
        "  text_massive = []\n",
        "  segments = res['segments']\n",
        "  for segment in segments:\n",
        "    fract1 = str(round(segment['start'] % 1, 3))\n",
        "    fract2 = str(round(segment['end'] % 1, 3))\n",
        "    start_time = str(0) + str(timedelta(seconds = int(segment['start']))) + ',' + fract1[fract1.find('.') + 1:].ljust(3, '0')\n",
        "    end_time = str(0) + str(timedelta(seconds = int(segment['end']))) + ',' + fract2[fract2.find('.') + 1:].ljust(3, '0')\n",
        "    text = segment['text']\n",
        "    id = segment['id']\n",
        "    seg = f\"{id + 1}\\n{start_time} --> {end_time}\\n{text[1:] if text and text[0] == ' ' else 'EMPTY' if text == '' else text}\"\n",
        "    text_massive.append(seg)\n",
        "  file.write(text_massive[0] + '\\n')\n",
        "  for txt_str in text_massive[1:-1]:\n",
        "    file.write('\\n' + txt_str + '\\n')\n",
        "  file.write('\\n' + text_massive[-1])\n",
        "  file.close()\n",
        "\n",
        "\n",
        "def write_words_to_file(res, path_to_text):\n",
        "  file = open(path_to_text, 'w')\n",
        "  text_massive = []\n",
        "  segments = res['segments']\n",
        "  index_shift = 0\n",
        "  for segment in segments:\n",
        "    words = segment['words']\n",
        "    for idx, word in enumerate(words):\n",
        "      fract1 = str(round(word['start'] % 1, 3))\n",
        "      fract2 = str(round(word['end'] % 1, 3))\n",
        "      start_time = str(0) + str(timedelta(seconds = int(word['start']))) + ',' + fract1[fract1.find('.') + 1:].ljust(3, '0')\n",
        "      end_time = str(0) + str(timedelta(seconds = int(word['end']))) + ',' + fract1[fract1.find('.') + 1:].ljust(3, '0')\n",
        "      text = word['word']\n",
        "      seg = f\"{index_shift + idx + 1}\\n{start_time} --> {end_time}\\n{text[1:] if text and text[0] == ' ' else 'EMPTY' if text == '' else text}\"\n",
        "      text_massive.append(seg)\n",
        "    index_shift += idx\n",
        "  file.write(text_massive[0] + '\\n')\n",
        "  for txt_str in text_massive[1:-1]:\n",
        "    file.write('\\n' + txt_str + '\\n')\n",
        "  file.write('\\n' + text_massive[-1])\n",
        "  file.close()\n",
        "\n",
        "\n",
        "def check_punctuation_percent(path, lang, subtitles):\n",
        "  punct = '.:!?,-‚Äî'\n",
        "  ok = True\n",
        "  punct_symbols = 0\n",
        "  other_symbols = 0\n",
        "\n",
        "  if not subtitles:\n",
        "    with open(path, 'r') as txtfile:\n",
        "      text = txtfile.read()\n",
        "    for char in text:\n",
        "      if char in punct:\n",
        "        punct_symbols += 1\n",
        "      else:\n",
        "        other_symbols += 1\n",
        "  else:\n",
        "    with open(path, 'r') as txtfile:\n",
        "      lines = [''] + txtfile.read().split('\\n')\n",
        "    for i in range(3, len(lines), 4):\n",
        "      for char in lines[i]:\n",
        "        if char in punct:\n",
        "          punct_symbols += 1\n",
        "        else:\n",
        "          other_symbols += 1\n",
        "\n",
        "  percent = punct_symbols / other_symbols * 100\n",
        "  if lang == 'en':\n",
        "    if percent < 1.3:\n",
        "      ok = False\n",
        "  elif lang == 'ru':\n",
        "    if percent < 2:\n",
        "      ok = False\n",
        "  return ok\n",
        "\n",
        "\n",
        "# –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≥–æ, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ —Å—Ä–µ–¥–µ GPU\n",
        "device_name = gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–æ–≤–µ—Ç: –ø–æ–¥–∫–ª—é—á–∏—Ç–µ—Å—å –∫ —Å—Ä–µ–¥–µ —Å GPU.')\n",
        "\n",
        "model_name = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\"]\n",
        "# –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "# –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ\n",
        "yt_downloaded = False\n",
        "gd_mounted = False\n",
        "\n",
        "path_to_video = '/content/vid.mp4'\n",
        "\n",
        "!rm rf /content/sample_data # —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω–æ–π –ø–∞–ø–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—ë—Ç—Å—è –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "2Gb2S89T48bJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ"
      ],
      "metadata": {
        "id": "QLPoBz2U23Eq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vwSvZa2x0kFV"
      },
      "outputs": [],
      "source": [
        "# @title –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –µ—Å–ª–∏ –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ—Ç–µ Google –î–∏—Å–∫:\n",
        "#@markdown ### –°–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏\n",
        "upload_method = \"–° —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\" #@param [\"–° —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\", \"–ü–æ —Å—Å—ã–ª–∫–µ —Å YouTube\", \"–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É Google Drive\"]\n",
        "#@markdown <font color=\"orange\"> –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –∏ —Å–æ–≤–µ—Ç—ã:\n",
        "\n",
        "#@markdown <font color=\"orange\"> ``1. –ü—Ä–∏ –≤—ã–±–æ—Ä–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –Ω–∞–∂–∞—Ç—å –Ω–∞ –∫–Ω–æ–ø–∫—É –≤ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–±—Ä–∞—Ç—å —Ñ–∞–π–ª —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.``\n",
        "\n",
        "#@markdown <font color=\"orange\"> ``2. –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å YouTube —É–∫–∞–∂–∏—Ç–µ —Å—Å—ã–ª–∫—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ.``\n",
        "\n",
        "#@markdown <font color=\"orange\"> ``3. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Å–ø–æ—Å–æ–±–∞, —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ —Å Google –î–∏—Å–∫–æ–º, —Å–∏—Å—Ç–µ–º–∞ –ø–æ–ø—Ä–æ—Å–∏—Ç –≤–∞—Å –¥–∞—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç—É–ø –∫ –≤–∞—à–µ–º—É Google Drive. –ó–∞—Ç–µ–º –æ–Ω –±—É–¥–µ—Ç —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –≤ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º –ø–æ–ª–µ.``\n",
        "\n",
        "#@markdown ``–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫ —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã YouTube –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ `` üëá\n",
        "youtube_url = 'https://www.youtube.com/watch?v=YOUTUBE_ID' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ``–í–≤–µ–¥–∏—Ç–µ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ –Ω–∞ –≤–∞—à–µ–º Google –î–∏—Å–∫–µ (–¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π —Å Google Drive) `` üëá\n",
        "path_google_drive = '/content/drive/MyDrive/path_to_vid' #@param {type:\"string\"}\n",
        "\n",
        "from urllib import parse as urlparse\n",
        "\n",
        "# –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ —Ç–∞–∫–∏–µ —É–∂–µ –±—ã–ª–∏ –≤ –§–° –ø—Ä–µ–∂–¥–µ\n",
        "if os.path.isfile(path_to_video):\n",
        "    os.remove(path_to_video)\n",
        "if os.path.isfile('/content/aud.wav'):\n",
        "    os.remove('/content/aud.wav')\n",
        "\n",
        "path_to_video = '/content/vid.mp4' # –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "\n",
        "# –í–∏–¥–µ–æ—Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏\n",
        "if upload_method == \"–° —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\":\n",
        "  uploaded = files.upload()\n",
        "  for filename in uploaded.keys():\n",
        "    if '/content/' + filename != path_to_video:\n",
        "      os.rename(filename, path_to_video)  # –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
        "elif upload_method == \"–ü–æ —Å—Å—ã–ª–∫–µ —Å YouTube\":\n",
        "  if not yt_downloaded:\n",
        "    !pip install yt-dlp # —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ\n",
        "    yt_downloaded = True\n",
        "  url_data = urlparse.urlparse(youtube_url)\n",
        "  query = urlparse.parse_qs(url_data.query)\n",
        "  YOUTUBE_ID = query[\"v\"][0]\n",
        "  # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ —Å YouTube\n",
        "  !yt-dlp -f \"bestvideo[ext=mp4]+bestaudio[ext=wav]/best[ext=mp4]/best\" --output \"/content/vid.%(ext)s\" https://www.youtube.com/watch?v=$YOUTUBE_ID\n",
        "  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
        "  for file in os.listdir('/content'):\n",
        "    if file.startswith('vid'):\n",
        "        path_to_video = os.path.join('/content', file)\n",
        "else:\n",
        "  if not gd_mounted:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\") # –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Google –î–∏—Å–∫–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ\n",
        "    gd_mounted = True\n",
        "  if not os.path.isfile(path_google_drive): # –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω\n",
        "      print(\"ERROR: File not found!\")\n",
        "      raise SystemExit(0)\n",
        "  !cp $path_google_drive $path_to_video # –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å Google –î–∏—Å–∫–∞\n",
        "\n",
        "# –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∏ –∏–∑ –≤–∏–¥–µ–æ\n",
        "video = VideoFileClip(path_to_video)\n",
        "video.audio.write_audiofile('/content/aud.wav') # –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∏\n",
        "\n",
        "clear_output() # –æ—á–∏—Å—Ç–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ (+ –ø–µ—Ä–µ–≤–æ–¥ –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞)"
      ],
      "metadata": {
        "id": "QZQsPqHvTlJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏\n",
        "\n",
        "# @markdown –í–≤–µ–¥–∏—Ç–µ –ø–æ–¥—Å–∫–∞–∑–∫—É –¥–ª—è –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ —ç—Ç–æ –∑–∞ –ª–µ–∫—Ü–∏—è.\n",
        "prompt = 'The text below is a lecture on economics.' #@param {type: \"string\"}\n",
        "\n",
        "# @markdown –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á—å—é?\n",
        "download_file = False # @param {type: \"boolean\"}\n",
        "\n",
        "if os.path.isfile('/content/aud.wav'):\n",
        "  result = model.transcribe('/content/aud.wav', initial_prompt = prompt)\n",
        "  lang = result['language']\n",
        "\n",
        "  txt_file = open('/content/result.txt', 'w')\n",
        "  txt_file.write(result['text'])\n",
        "  txt_file.close()\n",
        "\n",
        "  clear_output()\n",
        "  # –≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º\n",
        "  if lang == 'ru' or lang == 'en':\n",
        "    ok = check_punctuation_percent('/content/result.txt', lang, False)\n",
        "    assert ok, f\" [!] –í–µ—Ä–æ—è—Ç–Ω–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∞ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.\"\n",
        "\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏ /content/result.txt')\n",
        "  if download_file:\n",
        "    files.download('/content/result.txt')\n",
        "else:\n",
        "  print('–í—ã –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –≤–∏–¥–µ–æ. –í–µ—Ä–Ω–∏—Ç–µ—Å—å –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —è—á–µ–π–∫–µ.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vo1YDDcUT15z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –ü–µ—Ä–µ–≤–æ–¥\n",
        "\n",
        "# @markdown –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏:\n",
        "path_to_text = '/content/result.txt' # @param {type: \"string\"}\n",
        "\n",
        "!pip install -U deep-translator\n",
        "!pip install iso639 # –¥–ª—è –∫–æ–¥–æ–≤ —è–∑—ã–∫–æ–≤\n",
        "from deep_translator import GoogleTranslator\n",
        "import iso639\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#@markdown –í–≤–µ–¥–∏—Ç–µ —è–∑—ã–∫, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –∂–µ–ª–∞–µ—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –∏–∑ –≤–∏–¥–µ–æ\n",
        "language = \"english\" #@param {type: \"string\"}\n",
        "language = language.lower()\n",
        "lang_capital = language[0].upper() + language[1 :]\n",
        "\n",
        "# –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —è–∑—ã–∫–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º\n",
        "langs_list = GoogleTranslator().get_supported_languages()\n",
        "if language in langs_list: # –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫ –µ—Å—Ç—å –≤ —ç—Ç–æ–º —Å–ø–∏—Å–∫–µ\n",
        "  language = iso639.to_iso639_1(lang_capital) # –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ —è–∑—ã–∫–∞ ISO639-1\n",
        "  path_to_tr_text = '/content/result_' + language + '.txt'\n",
        "  # —Å–æ–∑–¥–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ\n",
        "  output_file = open(path_to_tr_text, \"w\")\n",
        "  txtfile = open(path_to_text, \"r\")\n",
        "  text = txtfile.read()\n",
        "  # –ø–µ—Ä–µ–≤–æ–¥\n",
        "  translated = GoogleTranslator(source='auto', target = language).translate(text)\n",
        "  output_file.write(translated)\n",
        "  txtfile.close()\n",
        "  output_file.close()\n",
        "  clear_output()\n",
        "\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏ ', path_to_tr_text)\n",
        "  print('–û–Ω –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ –≤–∞—à –∫–æ–º–ø—å—é—Ç–µ—Ä.')\n",
        "  # –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä\n",
        "  files.download(path_to_tr_text)\n",
        "else:\n",
        "  print(\"–î–∞–Ω–Ω—ã–π —è–∑—ã–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s0pJrvdicTYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ - —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ (+ –∏—Ö –ø–µ—Ä–µ–≤–æ–¥)"
      ],
      "metadata": {
        "id": "O0NPwPS-edjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏\n",
        "\n",
        "# @markdown –í–≤–µ–¥–∏—Ç–µ –ø–æ–¥—Å–∫–∞–∑–∫—É –¥–ª—è –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ —ç—Ç–æ –∑–∞ –ª–µ–∫—Ü–∏—è. –ò–ª–∏ –∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–æ–≤.\n",
        "prompt = 'The text below, consisting of segments ending with punctuation marks, is a lecture on economics.' #@param {type: \"string\"}\n",
        "# @markdown –ù—É–∂–Ω—ã –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤?\n",
        "\n",
        "# @markdown <font color=\"orange\"> –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: ``–≠—Ç–æ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –æ–±–µ—Å–ø–µ—á–∏—Ç –±–æ–ª—å—à—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤ —Å—É–±—Ç–∏—Ç—Ä–∞—Ö.``\n",
        "word_timestamps = True # @param {type: \"boolean\"}\n",
        "# @markdown –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á—å—é?\n",
        "download_file = False # @param {type: \"boolean\"}\n",
        "\n",
        "if os.path.isfile('/content/aud.wav'):\n",
        "  result = model.transcribe('/content/aud.wav', initial_prompt = prompt, word_timestamps = word_timestamps)\n",
        "  lang = result['language']\n",
        "\n",
        "  write_result_to_file(result, '/content/result.srt')\n",
        "  if word_timestamps:\n",
        "    path_to_words = '/content/words.txt'\n",
        "    write_words_to_file(result, path_to_words)\n",
        "    word_lines = []\n",
        "    with open(path_to_words, 'r') as wf:\n",
        "      word_lines = [''] + wf.read().split('\\n')\n",
        "\n",
        "  if lang == 'ru' or lang == 'en':\n",
        "    ok = check_punctuation_percent('/content/result.srt', lang, True)\n",
        "    assert ok, f\" [!] –í–µ—Ä–æ—è—Ç–Ω–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∞ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.\"\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏ /content/result.srt')\n",
        "  if download_file:\n",
        "    files.download('/content/result.srt')\n",
        "else:\n",
        "  print('–í—ã –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –≤–∏–¥–µ–æ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–µ—Ä–Ω–∏—Ç–µ—Å—å –∫ —è—á–µ–π–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6yy4kP5yekXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "\n",
        "#  @markdown –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã:\n",
        "file_path = '/content/result.srt' # @param {type: \"string\"}\n",
        "# @markdown –í–≤–µ–¥–∏—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∞–µ–º–æ–π —Ñ—Ä–∞–∑—ã (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö):\n",
        "max_dur = 25 # @param {type: \"integer\"}\n",
        "# @markdown –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–∞–π–ª —Å–æ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫?\n",
        "word_timestamps = True # @param {type: \"boolean\"}\n",
        "# @markdown –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏?\n",
        "download_file = False # @param {type: \"boolean\"}\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# –≤–∞–∂–Ω—ã–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –Ω–∏–∂–µ–ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π\n",
        "part_1 = 12 # –¥–æ –∫–∞–∫–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å —Ç–∞–π–º–∏–Ω–≥–∞ (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
        "part_2 = 17 # —Å –∫–∞–∫–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å —Ç–∞–π–º–∏–Ω–≥–∞\n",
        "\n",
        "\n",
        "# —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ —Å–ª–æ–≤\n",
        "def compromise_time(time1, time2, s1, s2):\n",
        "  coef = len(s1) / len(s1 + s2)\n",
        "  return time_to_str(str_to_time(time1) + (str_to_time(time2) - str_to_time(time1)) * coef)\n",
        "\n",
        "\n",
        "# –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –ø–æ —Å–ª–æ–≤–∞–º\n",
        "def words_time(time1, time2, s1, s2, word_lines):\n",
        "  time1_time = str_to_time(time1)\n",
        "  time2_time = str_to_time(time2)\n",
        "  space1 = s1.rfind(' ')\n",
        "  space2 = s2.find(' ')\n",
        "  word1 = s1[space1 + 1:] if space1 != -1 else s1\n",
        "  word2 = s2[:space2] if space2 != -1 else s2\n",
        "  #print(word1)\n",
        "  #print(word2)\n",
        "\n",
        "  new_end = 0\n",
        "  new_start = 0\n",
        "\n",
        "  for i in range(0, len(word_lines), 4):\n",
        "    if i + 1 < len(word_lines):\n",
        "      if word_lines[i+3] == word1 and word_lines[i+7] == word2:\n",
        "        # –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª–æ–≤ –¥–∞–Ω–Ω–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É\n",
        "        start_2_word_time = str_to_time(word_lines[i+6][:part_1])\n",
        "        if start_2_word_time > time1_time:\n",
        "          end_1_word_time = str_to_time(word_lines[i+2][part_2:])\n",
        "          # –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª—É—á–∞–π, —á—Ç–æ –∫–æ–Ω–µ—Ü –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –±—É–¥–µ—Ç –ø–æ–∑–∂–µ –Ω–∞—á–∞–ª–∞ –≤—Ç–æ—Ä–æ–≥–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º\n",
        "          if end_1_word_time > start_2_word_time:\n",
        "            start_1_word_time = str_to_time(word_lines[i+2][:part_1])\n",
        "            new_end = start_1_word_time\n",
        "          else:\n",
        "            new_end = end_1_word_time\n",
        "          new_start = start_2_word_time\n",
        "          # –æ–±—Ä–µ–∑–∞–µ–º –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞\n",
        "          words_num = len(s2.split()) # –ø–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤\n",
        "          word_lines = word_lines[i + (words_num + 1) * 4:]\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "  if not new_end:\n",
        "    print('–í–æ–∑–Ω–∏–∫ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª—É—á–∞–π. –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –ø–æ–ø–∞–ª–æ—Å—å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Å–ª–æ–≤–æ!')\n",
        "    print('–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤—Ä—É—á–Ω—É—é –º–µ–∂–¥—É ', time1, ' –∏ ', time2)\n",
        "    print('–í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω—É–ª–µ–≤—ã–º–∏.')\n",
        "\n",
        "  return time_to_str(new_end), time_to_str(new_start), word_lines\n",
        "\n",
        "\n",
        "# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞\n",
        "def proc_segm(new_lines, lines, current_sentence, start_timing, id, start, end, max_dur, ind, i):\n",
        "  # –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –≤—ã—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥. –ø—Ä–µ–¥–ª. –∏ –Ω–æ–≤–æ–µ –¥–æ —Ç–æ—á–∫–∏\n",
        "  if str_to_time(end) - str_to_time(start_timing) <= max_dur:\n",
        "    current_sentence += ' ' + lines[i + 3][:ind + 1]\n",
        "    new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + end + '\\n' + current_sentence)\n",
        "    id += 1\n",
        "  # –∏–Ω–∞—á–µ –Ω—É–∂–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –ø—Ä–µ–¥. –ø—Ä–µ–¥–ª. –∏ –∑–∞—Ç–µ–º —Ç–æ –¥–æ —Ç–æ—á–∫–∏\n",
        "  else:\n",
        "    new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + lines[i-2][part_2:] + '\\n' + current_sentence)  # –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–∞—Å—Ç–∏\n",
        "    new_lines.append('\\n' + str(id + 1) + '\\n' + lines[i+2][:part_1] + ' --> ' + end + '\\n' + lines[i + 3][:ind + 1])\n",
        "    id += 2\n",
        "  # –±–µ—Ä—ë—Ç—Å—è –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "  current_sentence = lines[i + 3][ind + 2:]\n",
        "  start_timing = start\n",
        "\n",
        "  return new_lines, current_sentence, start_timing, id\n",
        "\n",
        "\n",
        "# —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏\n",
        "def process_text(file_path, max_sent_dur, word_times, word_lines):\n",
        "    with open(file_path, 'r') as txtfile:\n",
        "        lines = [''] + txtfile.read().split('\\n')\n",
        "\n",
        "    new_lines = []\n",
        "    current_sentence = \"\"\n",
        "    symbols = '.!?'\n",
        "    start_timing = ''\n",
        "\n",
        "    id = 1\n",
        "\n",
        "    for i in range(0, len(lines), 4):\n",
        "      if i + 1 < len(lines):\n",
        "        time_match = re.match(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', lines[i+2])\n",
        "        text_match = re.match(r'[A-Z–ê-–Ø].*[.?!]$', lines[i+3])\n",
        "        start_match = re.match(r'[A-Z–ê-–Ø].*', lines[i+3])\n",
        "        end_match = re.match(r'.*[.?!].*', lines[i+3]) #[a-z–∞-—è]\n",
        "\n",
        "        if time_match:\n",
        "          # –ø–µ—Ä–≤—ã–π —Å–ª—É—á–∞–π\n",
        "          if (text_match or start_match) and start_timing == '': # —Ü–µ–ª–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–∑–∞–≥–ª. –±—É–∫–≤–∞ –¥–µ–π—Å—Ç–≤. —ç—Ç–æ –∑–Ω–∞—á–∏—Ç)\n",
        "              current_sentence = lines[i+3]\n",
        "              if text_match:\n",
        "                new_lines.append('\\n' + str(id) + '\\n' + lines[i+2] + '\\n' + current_sentence) # –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è\n",
        "                id += 1\n",
        "                current_sentence = ''\n",
        "                start_timing = ''\n",
        "              else: # –µ—Å–ª–∏ –∏–º–µ–µ—Ç –Ω–∞—á–∞–ª–æ, –Ω–æ –Ω–µ –∏–º–µ–µ—Ç –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "                start_timing = lines[i+2][:part_1] # –Ω–∞—á–∞–ª–æ —Ç–∞–π–º–∏–Ω–≥–∞, –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ\n",
        "          elif text_match: # —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–ª–µ–¥. —á–∞—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –Ω–æ —ç—Ç–æ –Ω–µ —Ü–µ–ª–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –ê –Ω–∞ –∫–æ–Ω—Ü–µ .?!\n",
        "            # —Ç. –µ. —ç—Ç–æ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ, –Ω–æ —Å –∑–∞–≥–ª. –±—É–∫–≤–æ–π\n",
        "            end_timing = lines[i+2][part_2:]\n",
        "            new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + end_timing + '\\n' + current_sentence + ' ' + lines[i+3])\n",
        "            current_sentence = ''\n",
        "            start_timing = ''\n",
        "\n",
        "          # –µ—Å–ª–∏ —á–∞—Å—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ–Ω–µ—á–Ω–æ–π, –Ω–∞—á–∏–Ω–∞—è—Å—å —Å–æ —Å—Ç—Ä–æ—á–Ω–æ–π –±—É–∫–≤—ã –∏ –∏–º–µ—è –≤ —Å–æ—Å—Ç–∞–≤–µ –∑–Ω–∞–∫ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "          elif end_match:\n",
        "            index = next((lines[i+3].find(char) for char in symbols if char in lines[i+3]), -1) # –ø–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å —Å–∏–º–≤–æ–ª–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "            end_timing = lines[i+2][part_2:] # –∫–æ–Ω–µ—á–Ω—ã–π —Ç–∞–π–º–∏–Ω–≥ —ç—Ç–æ–π —Ñ—Ä–∞–∑—ã\n",
        "            if lines[i+3][-1] not in symbols: # –µ—Å–ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –Ω–µ –æ–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Å–∏–º–≤–æ–ª–æ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è, —Ç. –µ. –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
        "              # —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º, –∫–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –∏ –∫–æ–≥–¥–∞ –Ω–∞—á–Ω—ë—Ç—Å—è –≤—Ç–æ—Ä–æ–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
        "              if word_times: # –µ—Å–ª–∏ —Å –º–µ—Ç–∫–∞–º–∏ —Å–ª–æ–≤\n",
        "                new_end, new_start, word_lines = words_time(lines[i+2][:part_1], end_timing, lines[i+3][:index + 1], lines[i+3][index + 2:], word_lines)\n",
        "                new_lines, current_sentence, start_timing, id = proc_segm(new_lines, lines, current_sentence,\n",
        "                                                                          start_timing, id, new_start, new_end,\n",
        "                                                                          max_dur, index, i)\n",
        "              else: # –µ—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "                between_timing = compromise_time(lines[i+2][:part_1], end_timing, lines[i+3][:index + 1], lines[i+3][index + 2:])\n",
        "                new_lines, current_sentence, start_timing, id = proc_segm(new_lines, lines, current_sentence,\n",
        "                                                                          start_timing, id, between_timing,\n",
        "                                                                          between_timing, max_dur, index, i)\n",
        "            else: # —Ñ—Ä–∞–≥–º–µ–Ω—Ç –æ–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Å–∏–º–≤–æ–ª–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\n",
        "              new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + end_timing + '\\n' + current_sentence + ' ' + lines[i + 3])\n",
        "              id += 1\n",
        "              current_sentence = ''\n",
        "              start_timing = ''\n",
        "\n",
        "          # –µ—Å–ª–∏ —á–∞—Å—Ç—å –Ω–µ –∏–º–µ–µ—Ç –Ω–∏ –Ω–∞—á–∞–ª–∞, –Ω–∏ –∫–æ–Ω—Ü–∞, –Ω–æ –µ—ë –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π\n",
        "          elif start_timing:\n",
        "            if str_to_time(lines[i+2][part_2:]) - str_to_time(start_timing) <= max_sent_dur:\n",
        "              current_sentence += ' ' + lines[i + 3]\n",
        "            else: # –Ω–æ —Ñ—Ä–∞–∑–∞ –æ–∫–∞–∑–∞–∑–∞–ª–∞—Å—å –¥–ª–∏–Ω–Ω–µ–µ, —á–µ–º –Ω–∞–¥–æ => –Ω–µ –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å\n",
        "              new_lines.append('\\n' + str(id) + '\\n' + start_timing + ' --> ' + lines[i - 2][part_2:] + '\\n' + current_sentence) # –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–∞—Å—Ç–∏\n",
        "              id += 1\n",
        "              # –ë—É–¥–µ–º –ø—Ä–∏–±–∞–≤–ª—è—Ç—å –∫ —Ç–µ–∫—É—â–µ–º—É —Å–µ–≥–º–µ–Ω—Ç—É –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ\n",
        "              start_timing = lines[i+2][:part_1]\n",
        "              current_sentence = lines[i+3]\n",
        "\n",
        "    return new_lines\n",
        "\n",
        "\n",
        "path_to_words = '/content/words.txt'\n",
        "word_lines = []\n",
        "if word_timestamps:\n",
        "  with open(path_to_words, 'r') as wf:\n",
        "    word_lines = [''] + wf.read().split('\\n')\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "new_lines = process_text(file_path, max_dur, word_timestamps, word_lines)\n",
        "\n",
        "txt_file = open('/content/subtitles.srt', \"w\")\n",
        "txt_file.write(new_lines[0][1:] + '\\n') # –±–µ–∑ —Å–∏–º–≤–æ–ª–∞ –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É\n",
        "if len(new_lines) > 1:\n",
        "  for line in new_lines[1:-1]:\n",
        "      txt_file.write(line + '\\n')\n",
        "  txt_file.write(new_lines[-1])\n",
        "txt_file.close()\n",
        "\n",
        "print('–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ /content/subtitles.srt')\n",
        "if download_file:\n",
        "  files.download('/content/subtitles.srt')"
      ],
      "metadata": {
        "id": "iedlpqdnkocO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –ü–µ—Ä–µ–≤–æ–¥\n",
        "\n",
        "# @markdown –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏:\n",
        "path_to_text = '/content/subtitles.srt' # @param {type: \"string\"}\n",
        "\n",
        "!pip install -U deep-translator\n",
        "!pip install iso639 # –¥–ª—è –∫–æ–¥–æ–≤ —è–∑—ã–∫–æ–≤\n",
        "from deep_translator import GoogleTranslator\n",
        "import iso639\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#@markdown –í–≤–µ–¥–∏—Ç–µ —è–∑—ã–∫, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –∂–µ–ª–∞–µ—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –∏–∑ –≤–∏–¥–µ–æ\n",
        "language = \"english\" #@param {type: \"string\"}\n",
        "language = language.lower()\n",
        "lang_capital = language[0].upper() + language[1 :]\n",
        "\n",
        "# –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —è–∑—ã–∫–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º\n",
        "langs_list = GoogleTranslator().get_supported_languages()\n",
        "if language in langs_list: # –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫ –µ—Å—Ç—å –≤ —ç—Ç–æ–º —Å–ø–∏—Å–∫–µ\n",
        "  language = iso639.to_iso639_1(lang_capital) # –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ —è–∑—ã–∫–∞ ISO639-1\n",
        "  path_to_tr_text = '/content/subtitles_' + language + '.srt'\n",
        "  # —Å–æ–∑–¥–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ\n",
        "  output_file = open(path_to_tr_text, \"w\")\n",
        "  txtfile = open(path_to_text, \"r\")\n",
        "  lines = [''] + txtfile.read().split('\\n')\n",
        "  for i, line in enumerate(lines):\n",
        "    if i != len(lines) - 1:\n",
        "      if i % 4 == 3:\n",
        "        translated = GoogleTranslator(source='auto', target = language).translate(line)\n",
        "        output_file.write(translated + \"\\n\")\n",
        "      else:\n",
        "        if i != 0:\n",
        "          output_file.write(line + \"\\n\")\n",
        "    else:\n",
        "      translated = GoogleTranslator(source='auto', target = language).translate(line)\n",
        "      output_file.write(translated)\n",
        "  txtfile.close()\n",
        "  output_file.close()\n",
        "  clear_output()\n",
        "\n",
        "  print('–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ –ø—É—Ç–∏', path_to_tr_text)\n",
        "  print('–û–Ω –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ –≤–∞—à –∫–æ–º–ø—å—é—Ç–µ—Ä.')\n",
        "  files.download(path_to_tr_text)\n",
        "else:\n",
        "  print(\"–î–∞–Ω–Ω—ã–π —è–∑—ã–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º.\")"
      ],
      "metadata": {
        "id": "D1hhwyma0A55",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –†–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å Google Drive"
      ],
      "metadata": {
        "id": "U8Jqe4Iz1NqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "0-ykmtHc1X6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}